{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize, to_pil_image  # type: ignore\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, annotation_file, transform=None):\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "        # filter out image_ids without any annotations\n",
    "        self.image_ids = [image_id for image_id in self.image_ids if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, image_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Failed to load image: {image_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_height, image_width, _ = image.shape # (height, width, 3)\n",
    "        \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x+w, y+h])\n",
    "            # mask = self.coco.annToMask(ann)\n",
    "            # get binary mask of each bbox\n",
    "            mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "            mask[y:y+h, x:x+w] = 1 # or mask[y:y+h+1, x:x+w+1] = 1\n",
    "            masks.append(mask)\n",
    "            \n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        \n",
    "        return image, torch.tensor(bboxes), torch.tensor(masks).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeLongestSide:\n",
    "    \"\"\"\n",
    "    Resizes images to the longest side 'target_length', as well as provides\n",
    "    methods for resizing coordinates and boxes. Provides methods for\n",
    "    transforming both numpy array and batched torch tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_length: int) -> None:\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def apply_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        target_size = self.get_preprocess_shape(image.shape[0], image.shape[1], self.target_length)\n",
    "        return np.array(resize(to_pil_image(image), target_size))\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], self.target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "    \n",
    "    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array shape Bx4. Requires the original image size\n",
    "        in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    def apply_image_torch(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects batched images with shape BxCxHxW and float format. This\n",
    "        transformation may not exactly match apply_image. apply_image is\n",
    "        the transformation expected by the model.\n",
    "        \"\"\"\n",
    "        # Expects an image in BCHW format. May not exactly match apply_image.\n",
    "        target_size = self.get_preprocess_shape(image.shape[2], image.shape[3], self.target_length)\n",
    "        return F.interpolate(\n",
    "            image, target_size, mode=\"bilinear\", align_corners=False, antialias=True\n",
    "        )\n",
    "        \n",
    "    def apply_coords_torch(\n",
    "        self, coords: torch.Tensor, original_size: Tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with length 2 in the last dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], self.target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).to(torch.float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes_torch(\n",
    "        self, boxes: torch.Tensor, original_size: Tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with shape Bx4. Requires the original image\n",
    "        size in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords_torch(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute the output size given input size and target long side length.\n",
    "        \"\"\"\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww = int(neww + 0.5)\n",
    "        newh = int(newh + 0.5)\n",
    "        return (newh, neww)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeAndPad:\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "        image = self.transform.apply_image(image)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        # pad image and masks to form a square (e.g. go from torch.Size([3, 215, 160]) to torch.Size([3, 215, 215]))\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "\n",
    "        # adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "        \n",
    "        return image, masks, bboxes\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    images, bboxes, masks = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, bboxes, masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "image_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "transform = ResizeAndPad(image_size)\n",
    "\n",
    "train = COCODataset(image_dir=\"/scratch/students/danae/data/images\", \n",
    "                    annotation_file=\"/scratch/students/danae/data/model_data_format/yolo/coco_annotations.json\",\n",
    "                    transform=transform)\n",
    "\n",
    "val = COCODataset(image_dir=\"/scratch/students/danae/data/images\", \n",
    "                  annotation_file=\"/scratch/students/danae/data/model_data_format/yolo/coco_annotations.json\",\n",
    "                  transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(train,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def get_bounding_box(self, bboxes):\n",
    "    x = [x.detach().item() for i in [0,2] for x in bboxes[:,i]]\n",
    "    y = [y.detach().item() for i in [1,3] for y in bboxes[:,i]]\n",
    "    x0, y0, x1, y1 = min(x), min(y), max(x), max(y)\n",
    "    assert x1 >= x0 and y1 >= y0\n",
    "    bbox = [x0, y0, x1, y1]\n",
    "    return bbox\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[0]\n",
    "    prompts = item[1] # bboxes\n",
    "    \n",
    "    # prepare prompt for the model\n",
    "    prompt = self.get_bounding_box(prompts) # 1 big bbox\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    masks = item[2]\n",
    "    inputs[\"ground_truth_mask\"] = torch.any(masks, dim=0).int()\n",
    "\n",
    "    return inputs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(dataset=train, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Note: hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                      multimask_output=False)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Training loop with validation\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # Compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # Backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch+1}')\n",
    "    print(f'Mean Training Loss: {mean(epoch_losses):.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_losses = []\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                            input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                            multimask_output=False)\n",
    "\n",
    "            # Compute loss\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    print(f'Mean Validation Loss: {mean(val_losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoSegmentationDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transforms=None):\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, img_info['filename'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # create an empty mask\n",
    "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "        # convert bounding boxes to masks\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # COCO format: [x, y, width, height]\n",
    "            x, y, w, h = map(int, bbox)\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), color=1, thickness=-1)\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, image_size=(512, 512)):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get item from dataset\n",
    "        item = self.dataset[idx]\n",
    "        image = item[0]  # assumes image is in PIL or array format\n",
    "        prompts = item[1]  # List of bounding boxes\n",
    "\n",
    "        # convert bounding boxes to prompts\n",
    "        input_boxes = self._convert_bboxes_to_prompts(prompts)\n",
    "\n",
    "        # Prepare image and prompts for the processor\n",
    "        inputs = self.processor(image, input_boxes=input_boxes, return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension added by the processor\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # Create ground truth segmentation mask\n",
    "        ground_truth_mask = self._create_ground_truth_map(prompts, image.size)\n",
    "\n",
    "        # Add ground truth mask to the inputs\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _convert_bboxes_to_prompts(self, bboxes):\n",
    "        \"\"\"\n",
    "        Converts bounding boxes into the format required by the processor.\n",
    "        Args:\n",
    "            bboxes: List of bounding boxes in [x, y, w, h] format.\n",
    "        Returns:\n",
    "            List of bounding boxes in [x_min, y_min, x_max, y_max] format.\n",
    "        \"\"\"\n",
    "        converted = [[x, y, x + w, y + h] for x, y, w, h in bboxes]\n",
    "        return converted\n",
    "\n",
    "    def _create_ground_truth_map(self, bboxes, image_size):\n",
    "        \"\"\"\n",
    "        Creates a binary mask where bounding box areas are marked as 1.\n",
    "        Args:\n",
    "            bboxes: List of bounding boxes in [x_min, y_min, x_max, y_max] format.\n",
    "            image_size: Tuple of (width, height) of the image.\n",
    "        Returns:\n",
    "            A PyTorch tensor representing the ground truth mask.\n",
    "        \"\"\"\n",
    "        mask = torch.zeros(image_size[1], image_size[0], dtype=torch.float32)  # (height, width)\n",
    "        for bbox in bboxes:\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "            mask[y_min:y_max, x_min:x_max] = 1.0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export CUDA_VISIBLE_DEVICES=0\n",
    "# nvidia-smi # ps aux | grep 1045526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAMDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SAMDataset(dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m      2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAMDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = SAMDataset(dataset=dataset[\"train\"], processor=processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                      multimask_output=False)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layout_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
